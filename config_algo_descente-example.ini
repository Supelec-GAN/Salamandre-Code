[Parametres de descente]

# Choisir l'algorithme de descente a utiliser parmi "Gradient", "Adagrad", "RMSProp", "Adadelta", "Adam"
algo_utilise = ["Gradient"]


# dans la descente du gradient avec ou sans momentum, et les algos Adagrad (valeur par defaut conseillee 0.1) et RMSProp (valeur par defaut conseillee 0.01)
eta = [1]

# Utilisable dans tous les algorithmes. Mettre a 0 pour travailler sans
momentum = [0.5]

# Sert dans tous les algorithmes sauf la descente de gradient. Petites valeurs (sert a avoir un denominateur non nul). Valeur par defaut pour Adam 10^-8
epsilon = [10e-8]

# Utile dans RMSProp et Adadelta. A valeurs dans [0,1]
gamma = [0.9]

# Utile dans les algorithmes RMSProp et Adadelta, a valeurs dans {1, 2} pour choisir quels moments utiliser
moment = [2]


#Parametres exclusifs a l'algorithme Adam. Valeurs par defaut : gamma_1 = 0.9 et gamma_2 = 0.999
alpha = [1]
gamma_1 = [0.5]           # a valeurs dans [0,1[
gamma_2 = [0.5]           # a valeurs dans [0,1[

# Truc tres sale ou on decide de la fonction d'erreur pour retropropager sur le reseau precedent ==> astuce pour le mnist : osef parce que pas de reseau precedent
error_function_gen = [NonSatHeuristic()]

[Param de desc du disc]

# La fonction d'erreur à utiliser dans le cas où c'est une backprop
error_function_gen = [NonSatHeuristic()]

# Choisir l'algorithme de descente a utiliser parmi "Gradient", "Adagrad", "RMSProp", "Adadelta", "Adam"
algo_utilise = ["Gradient"]


# dans la descente du gradient avec ou sans momentum, et les algos Adagrad (valeur par defaut conseillee 0.1) et RMSProp (valeur par defaut conseillee 0.01)
eta = [0.5]

# Utilisable dans tous les algorithmes. Mettre a 0 pour travailler sans
momentum = [0]

# Sert dans tous les algorithmes sauf la descente de gradient. Petites valeurs (sert a avoir un denominateur non nul). Valeur par defaut pour Adam 10^-8
epsilon = [10e-8]

# Utile dans RMSProp et Adadelta. A valeurs dans [0,1]
gamma = [0.9]

# Utile dans les algorithmes RMSProp et Adadelta, a valeurs dans {1, 2} pour choisir quels moments utiliser
moment = [2]


#Parametres exclusifs a l'algorithme Adam. Valeurs par defaut : gamma_1 = 0.9 et gamma_2 = 0.999
alpha = [1]
gamma_1 = [0.9]           # a valeurs dans [0,1[
gamma_2 = [0.999]           # a valeurs dans [0,1[

[Param de desc du gen]


# Choisir l'algorithme de descente a utiliser parmi "Gradient", "Adagrad", "RMSProp", "Adadelta", "Adam"
algo_utilise = ["Gradient"]


# dans la descente du gradient avec ou sans momentum, et les algos Adagrad (valeur par defaut conseillee 0.1) et RMSProp (valeur par defaut conseillee 0.01)
eta = [0.5]

# Utilisable dans tous les algorithmes. Mettre a 0 pour travailler sans
momentum = [0]

# Sert dans tous les algorithmes sauf la descente de gradient. Petites valeurs (sert a avoir un denominateur non nul). Valeur par defaut pour Adam 10^-8
epsilon = [10e-8]

# Utile dans RMSProp et Adadelta. A valeurs dans [0,1]
gamma = [0.9]

# Utile dans les algorithmes RMSProp et Adadelta, a valeurs dans {1, 2} pour choisir quels moments utiliser
moment = [2]


#Parametres exclusifs a l'algorithme Adam. Valeurs par defaut : gamma_1 = 0.9 et gamma_2 = 0.999
alpha = [1]
gamma_1 = [0.9]           # a valeurs dans [0,1[
gamma_2 = [0.999]           # a valeurs dans [0,1[
